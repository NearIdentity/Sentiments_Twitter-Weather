{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of Temporal and Meteorological Effects on Twitter Sentiments Across the United States : Towards Harnessing Data Science Tools for Monitoring Mental Health Issues in a Population\n",
    "\n",
    "## Preamble -- Using Data Science to Help Diagnose and Treat Mental Health Issues\n",
    "\n",
    "Mental health issues have generated a significant social stir due to their excruciating humane toll, the usual difficulty of their diagnosis, as well as the often-hidden socio-economic costs they inflict [1]. Social media provides rich, crowd-sourced data for a varierty of analytical projects, and may afford the scientfic commuity a useful vantage point for studying, diagnosing, and hopefully, mitigating mental health issues in a population. \n",
    "\n",
    "This project proposes to attempt the following:\n",
    "    1. Mine social media data on Twitter for a few of the most densely populated areas of USA.\n",
    "    2. Gather concurrent weather data for the above locations.\n",
    "    3. Analyse tweet sentiments to study the effects of location, weather, time, etc. \n",
    "    \n",
    "While this is only a small project borne out of the hobbies of a novice enthusiast in Data Science, it is hoped that this small prototype, in its own small way, will help raise awareness and interest over the the topic of mental health issues -- and encourage the use of science & technology to help diagnose/solve real-life problems. \n",
    "\n",
    "The motivation for this project to study social media sentiments and short-term environmental factors (weather, time of day, etc.) can be summed up by the following axioms: \n",
    "    1. We are products of our environment.\n",
    "    2. We can make conscious choices regarding what we think and how we act.\n",
    "    3. We can help make positive changes in the environment that shapes us.\n",
    "    \n",
    "In fact, axioms 2 and 3 above are often taken to be the basis of Cognitive-Behavioural Therapy (CBT) by the community of psychaitaric practitioners [2]. Once a fuller scientific understanding of social media sentiments is established, the power of this could be harnessed to deliver better public health interventions towards mental health issues. For example, Seasonal Affective Disorder (SAD) has well-established weather-based triggers [3]. Weather forecasts could be used to predict higher instances of this, social media data could be used to identify its occurrence, and perhaps, social media could also be used to offer support mechanisms for sufferers of SAD when their symptoms flare up. \n",
    "\n",
    "Hopefully, the potential of the Data Revolution that is upon us can be utilised to mitigate, if not alleviate, mental health afflictions in our society.\n",
    "    \n",
    "[1] \"The Neglect of Mental Illness Exacts a Huge Toll, Human and Economic\", Scientific American (2012): https://www.scientificamerican.com/article/a-neglect-of-mental-illness/\n",
    "\n",
    "[2] \"Cognitive-Behavioural Theray (CBT)\", Centre for Addiction and Mental Health, University of Toronto (Undated): http://www.camh.ca/en/hospital/health_information/a_z_mental_health_and_addiction_information/CBT/Pages/default.aspx\n",
    "\n",
    "[3] \"Seasonal Affective Disorder (SAD)\", The Mayo Clinic (Undated): http://www.mayoclinic.org/diseases-conditions/seasonal-affective-disorder/basics/definition/con-20021047\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to be Addressed\n",
    "\n",
    "    1. What are the different categories of weather to be found in USA?\n",
    "        * How does the geographical location of US cities correlate with weather data?\n",
    "        * What factors does weather depend upon?\n",
    "    2. How does the circadian rhythm affect the sentiment of tweets?\n",
    "    3. Which regions are most likely to be positive/negative in their tweets?\n",
    "    4. What are people tweeting about (and why)?\n",
    "\n",
    "## Overview of Tools and Strategies -- Twitter and OpenWeatherMap, Sentiment Analysis via AFINN and NLTK, SciKit Learn for Analytics (k-Means Clusters, PCA, LDA/NMF Topic Models), and Flask for Presentation of Topics\n",
    "\n",
    "Twiiter data will be mined using the API provided by Twitter Inc. While geo-tagged tweets would have been ideal, they are rarer and significantly more difficult to mine than regular tweets. For an initial prototype, the user profile location shall be taken as a placeholder for actual location.\n",
    "\n",
    "Weather data data will be obtained via the OpenWeatherMap API. \n",
    "\n",
    "Sentiment Analysis of the tweets will be undertaken using AFINN and NLTK.\n",
    "\n",
    "SciKit Learn will be used for general-purpose data analysis: k-Means Clustering, Principal Component Analysis (PCA), as well as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorisation (NMF).\n",
    "\n",
    "A small Flask application will also be written to facilitate the presentation of tweet topics by location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Project -- Files and Directories\n",
    "\n",
    "The root directory (https://github.com/NearIdentity/Sentiments_Twitter-Weather) contains the following Python script files:\n",
    "    \n",
    "* TwitterAcquisitionAnalysis.py \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/TwitterAcquisitionAnalysis.py)\n",
    "        * Uses the Twitter API to gather data over 24 hours by city and integrates them with weather data based on an OpenWeatherMap wrapper contained in another module. \n",
    "        * Ascribes sentiment scores to each tweet based on AFINN and NLTK \n",
    "    \n",
    "* OpenWeatherMapData.py \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/OpenWeatherMapData.py)\n",
    "Provides a wrapper API around the OpenWeatherMap API for use by the Twitter data acquisition/integration module contained in TwitterAcquisitionAnalysis.py\n",
    "    \n",
    "* PostProcesing.py \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/PostProcessing.py)\n",
    "This is where the bulk of the data analysis occurs. All code shown in this Jupyter notebook belongs to PostProcessing.py unless otherwise noted.\n",
    "    \n",
    "* AncilliaeHTML.py \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/AncilliaeHTML.py)\n",
    "Provides helper functions for creating HTML output for topic model data. These are to be later used to build a small web app in Flask\n",
    "\n",
    "* Analysis.py \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/Analysis.py)\n",
    "Provides helper functions for data analysis using SciKit Learn (Elbow Method plots for k-Means, PCA, as well as topic models, etc.).\n",
    "\n",
    "* AFINN-111.txt \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/AFINN-111.txt)\n",
    "AFINN data for sentiment scores [Details available at: http://www2.imm.dtu.dk/pubdb/view/publication_details.php?id=6010]\n",
    "\n",
    "* data/ \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/tree/master/data)\n",
    "Directory containing tweet data text with file-names of the form [<city>_full__pos|neg|neu.txt]; files are generated by TwitterAcquisitionAnalysis.py. Also contains image files for data plots created by PostProcessing.py\n",
    " \n",
    "* data/integrated_data_combined.csv \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/data/integrated_data_combined.csv)\n",
    "Data file generated by TwitterAcquisitionAnalysis.py containing tweet sentiments and weather data \n",
    "\n",
    "* FlaskApp/ \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/tree/master/FlaskApp)\n",
    "Directory containing the Flask app for presenting topic model data\n",
    "\n",
    "* FlaskApp/FlaskApp.py \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/FlaskApp/FlaskApp.py)\n",
    "Flask implementation of web app for displaying topic models and word clouds for different cities\n",
    "            \n",
    "* FlaskApp/TopicIndex.html \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/FlaskApp/TopicIndex.html)\n",
    "Main HTML index page for the Flask app; provides links to the toic models by city.\n",
    "            \n",
    "* FlaskApp/pages/ \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/tree/master/FlaskApp/pages)\n",
    "Directory containing the HTML pages to be used by the Flask app\n",
    "            \n",
    "* FlaskApp/pages/images/ \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/tree/master/FlaskApp/pages/images) \n",
    "Directory containing images of word clouds created for the HTML files\n",
    "                \n",
    "* pos.txt \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/pos.txt)\n",
    "Integrated data containing positive-sentiment tweets only from the cities polled\n",
    "\n",
    "* neg.txt \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/neg.txt)\n",
    "Integrated data containing negative-sentiment tweets only from the cities polled\n",
    "\n",
    "* neu.txt \n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/neu.txt)\n",
    "Integrated data containing neutral-sentiment tweets only from the cities polled\n",
    "\n",
    "* TwitterSentiments-Weather.ipynb\n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/TwitterSentiments-Weather.ipynb)\n",
    "This Jupyter notebook\n",
    "\n",
    "* TwitterSentiments-Weather.html\n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/TwitterSentiments-Weather.ipynb)\n",
    "HTML page based on this Jupyter notebook \n",
    "\n",
    "* TwitterSentiments-Weather_files/\n",
    "(https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/TwitterSentiments-Weather_files)\n",
    "Ancilliary files for HTML page based on this Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing of Twitter+Weather Data (PostProcessing.py)\n",
    "\n",
    "We have an integrated data file (https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/data/integrated_data_combined.csv) which contains tweet sentiment and local weather data. The file contains 41,000+ data-points collected over the span of about 24 hours from tweets sourced from the Twitter API and weather data from the OpenWeatherMap API. The data file was generated using TwitterAcquisitionAnalysis.py (https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/TwitterAcquisitionAnalysis.py) and we post-process said data using the script PostProcessing.py (https://github.com/NearIdentity/Sentiments_Twitter-Weather/blob/master/PostProcessing.py), which also serves as the basis of the present Jupyter notebook (with exceptions noted inline). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start off by getting the necessary modules and data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from os import getcwd, path, mkdir\n",
    "from Analysis import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "dataframe_full = pd.read_csv(getcwd()+\"/data/integrated_data_combined.csv\")\n",
    "list_headers = list(dataframe_full)\n",
    "array_coordinates = dataframe_full.values[:,0:2]\n",
    "array_sentiments = dataframe_full.values[:,3:8]\n",
    "array_weather0 = np.array(dataframe_full.values[:,8:13], dtype=float)\n",
    "set_weather1 = set(dataframe_full.values[:,13])\n",
    "array_weather1 = np.zeros((dataframe_full.values.shape[0], len(set_weather1)))\n",
    "array_weather2 = np.array(dataframe_full.values[:,14:17], dtype=float)\n",
    "array_weather3 = np.array(dataframe_full.values[:,18:19], dtype=float)\n",
    "array_weather4 = np.array(dataframe_full.values[:,19:21], dtype=float)\n",
    "array_day_night = np.zeros((dataframe_full.values.shape[0],2))\n",
    "array_phase24h = np.empty((dataframe_full.values.shape[0],1))\n",
    "\n",
    "list_headers_coordinates = list_headers[0:2]\n",
    "list_headers_sentiments = list_headers[3:8]\n",
    "list_headers_weather0 = list_headers[8:13]\n",
    "list_headers_weather1 = [list_headers[13]]\n",
    "list_headers_weather2 = list_headers[14:17]\n",
    "list_headers_weather3 = list_headers[18:19]\n",
    "list_headers_weather4 = list_headers[19:21]\n",
    "list_headers_day_night = [\"day(_)\", \"night(_)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means Clustering of Location (Elbow Method and Plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start off with analysing coordinates of the locations. This data was \"hard-coded\" into the .csv file while reading tweets. There are many repetitions of the same coordinates, but there should only be a few unique ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "=======================================================\n",
    "\tPre-Processing of Location Coordinates\n",
    "=======================================================\n",
    "'''\n",
    "\n",
    "list_coordinates = [str(coord[0])+' '+str(coord[1]) for coord in array_coordinates]\n",
    "set_coordinates = set(list_coordinates)\n",
    "array_coordinates_unique = np.array([[float(unq_coord.split()[0]),float(unq_coord.split()[1])] for unq_coord in set_coordinates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to use k-Means Clustering of the location coordinates with some plotting on a geographical map. We will use the Elbow Method to choose a good k-value. It turns out, the best k-value to use will be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceberg/anaconda2/lib/python2.7/site-packages/mpl_toolkits/basemap/__init__.py:3222: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0.\n",
      "  b = ax.ishold()\n",
      "/home/iceberg/anaconda2/lib/python2.7/site-packages/mpl_toolkits/basemap/__init__.py:3231: MatplotlibDeprecationWarning: axes.hold is deprecated.\n",
      "    See the API Changes document (http://matplotlib.org/api/api_changes.html)\n",
      "    for more details.\n",
      "  ax.hold(b)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "==================================================\n",
    "\tCluster Analysis of Locations\n",
    "==================================================\n",
    "'''\n",
    "\n",
    "kMeans_elbow_method_plot(array_coordinates_unique, 20, getcwd()+\"/data/Coordinates_kMeans_ElbowMethod.svg\")\n",
    "\n",
    "def create_background_map():\n",
    "\tplt.figure()\n",
    "\tbackground_map = Basemap(projection='stere', lat_0=+38.60, lon_0=-97.72, llcrnrlat=+22.00, urcrnrlat=+48.96, llcrnrlon=-122.68, urcrnrlon=-60.28, rsphere=6371200., resolution='l', area_thresh=10000)\n",
    "\tbackground_map.drawcoastlines()\n",
    "\tbackground_map.drawstates()                  \n",
    "\tbackground_map.drawcountries()\n",
    "\tparallels = np.arange(0.,90,10.)\n",
    "\tbackground_map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "\tmeridians = np.arange(180.,360.,10.)\n",
    "\tbackground_map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "\t\n",
    "\treturn background_map\n",
    "\n",
    "scaler_coordinates = StandardScaler()\n",
    "kBest_coordinates = 3\n",
    "list_colours = ['r', 'g', 'b']\n",
    "array_coordinates_scaled_unique = scaler_coordinates.fit_transform(array_coordinates_unique)\n",
    "kMeans_coordinates = kMeans_model(array_coordinates_scaled_unique, kBest_coordinates)\n",
    "array_kMeans_coordinate_labels = kMeans_coordinates[1]\n",
    "array_kMeans_coordinate_cen = kMeans_coordinates[2]\n",
    "\n",
    "array_kMeans_coordinate_colours = np.array([list_colours[label] for label in array_kMeans_coordinate_labels])\n",
    "\n",
    "array_latitudes = array_coordinates_unique[:, 0]\n",
    "array_longitudes = array_coordinates_unique[:, 1]\n",
    "bckgr_map = create_background_map()\n",
    "x, y = bckgr_map(array_longitudes, array_latitudes)\n",
    "bckgr_map.scatter(x, y, color=array_kMeans_coordinate_colours, s=200.0, marker='o', alpha=0.7)\n",
    "plt.savefig(getcwd()+\"/data/LocationClusters.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ignore the warning messages above. Let us see how plots of the Elbow Method and the location clusters turn out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method Plot for Optimal k-Means Clustering of Location Coordinates\n",
    "\n",
    "![title](data/Coordinates_kMeans_ElbowMethod.svg)\n",
    "\n",
    "The plot above seems to suggest an \"elbow\" at k=3.\n",
    "\n",
    "#### Geographical Illustration of Location Clusters\n",
    "\n",
    "![title](data/LocationClusters.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a geographical distribution of locations that roughly seem to be as follows:\n",
    "    * East Coast and Mid-West (red)\n",
    "    * West Coast and the Rocky Mountains (blue)\n",
    "    * South (green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to engineer the features of the weather data somewhat. Let us deviate from the script file PostProcessing.py somewhat and inspect what we have for the weather data. Previously, we had separated the headers and the data values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temp(C)', 'temp_max(C)', 'temp_min(C)', 'humidity(%)', 'pressure(hPa)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_headers_weather0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   19.88,    23.  ,    17.  ,    64.  ,  1015.  ],\n",
       "       [   16.68,    21.  ,    12.  ,    68.  ,  1015.  ],\n",
       "       [   19.88,    23.  ,    17.  ,    64.  ,  1015.  ],\n",
       "       [   14.72,    17.  ,    13.  ,    72.  ,  1018.  ],\n",
       "       [   24.92,    28.  ,    22.  ,    34.  ,  1011.  ],\n",
       "       [   16.68,    21.  ,    12.  ,    68.  ,  1015.  ],\n",
       "       [   12.2 ,    14.  ,    10.  ,   100.  ,  1019.  ],\n",
       "       [   24.37,    26.  ,    23.  ,    78.  ,  1014.  ],\n",
       "       [   23.88,    25.  ,    23.  ,    83.  ,  1013.  ],\n",
       "       [   25.37,    27.  ,    24.  ,    83.  ,  1012.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_weather0[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right. This is run-off-the-mill numerical data. Nothing out of the ordinary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky(__)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_headers_weather1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a description of the sky conditions (in words) and will need to be converted into one-hot vectors. Let us see what kind of values we are dealing with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Clear', 'Clouds', 'Drizzle', 'Fog', 'Haze', 'Mist', 'Rain', 'Thunderstorm'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_weather1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to the script, PostProcessing.py. Let us convert the descriptions of the sky that are mapped onto this seven-element set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "=============================================\n",
    "\tWeather Data Pre-Processing\t\n",
    "=============================================\n",
    "'''\n",
    "# One-Hot Vectors from Weather Descriptions\n",
    "\n",
    "dict_weather1 = {}\n",
    "i_weather1 = 0\n",
    "for desc_weather1 in set_weather1:\n",
    "\tdict_weather1[desc_weather1] = i_weather1\n",
    "\ti_weather1 += 1\n",
    "\n",
    "i_weather1 = 0\n",
    "for weather1 in dataframe_full.values[:,13]:\n",
    "\tarray_weather1[i_weather1, dict_weather1[weather1]] = 1\n",
    "\ti_weather1 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Angle Representation for Time of the Day -- Precursor for Studying the Impact of Circadian Rhythm on Tweet Sentiment Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to study the effect of circadian rhythm on the tweet sentiments. Now, the raw time-stamp of a tweet is not a good indicator of what phase of the day we are in, as the number of daylight/nightly hours vary throughout the United States. We circumvent this issue by mapping the time-stamp for each tweet into what we will refer to as the \"journal phase\", $\\delta$. The daytime values of this are designed to lie between 0 (at sunrise) and $+\\pi$ (at sunset). At sunset, the journal phase jumps from $+\\pi$ to $-\\pi$, causing overnight values to lie between $-\\pi$ (at sunset) and 0 (at sunrise).\n",
    "\n",
    "We also convert the time-stamp of every tweet into a day/night one-hot vector for easier analysis. We treat these as parts of the weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Day/Night One-Hot Vectors and Phase Angle\n",
    "\n",
    "array_time = dataframe_full.values[:,2]\n",
    "array_sunrise = dataframe_full.values[:,14]\n",
    "array_sunset = dataframe_full.values[:,15]\n",
    "for i_day_night in range(dataframe_full.values.shape[0]):\n",
    "\thours_day = array_sunset[i_day_night] - array_sunrise[i_day_night]\n",
    "\thours_night = 24.0 - hours_day\n",
    "\tif (array_time[i_day_night] == array_sunrise[i_day_night]):\t# Sunrise\n",
    "\t\tarray_phase24h[i_day_night,0] = 0\n",
    "\telif (array_time[i_day_night] == array_sunset[i_day_night]):\t# Sunset\n",
    "\t\tarray_phase24h[i_day_night,0] = np.pi\n",
    "\telif (array_sunrise[i_day_night] < array_time[i_day_night]) and (array_time[i_day_night] < array_sunset[i_day_night]):\t# Day\n",
    "\t\tarray_day_night[i_day_night,0] = 1.0\n",
    "\t\tarray_phase24h[i_day_night,0] = np.pi * (array_time[i_day_night] - array_sunrise[i_day_night])/hours_day\n",
    "\telse:\t# Night\n",
    "\t\tarray_day_night[i_day_night,1] = 1.0\n",
    "\t\tif array_time[i_day_night] < array_sunset[i_day_night]: # Past midnight\n",
    "\t\t\tarray_time[i_day_night] += 24.0\n",
    "\t\tarray_phase24h[i_day_night,0] = np.pi * (-1 + (array_time[i_day_night] - array_sunset[i_day_night])/hours_night)\n",
    "\t\tif array_time[i_day_night] >= 24.0: # Past midnight\n",
    "\t\t\tarray_time[i_day_night] -= 24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating Weather Data into a Single NumPy Array of Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a data scaler from SciKit Learn to the weather data in preparation for cluster analysis. However, we do not apply the scaler on the one-hot-vectorised data. \n",
    "\n",
    "We synthesise the resultant scaled data into a NumPy array for cluster analysis of weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaled Combination of Weather Data for Cluster Analysis\n",
    "\n",
    "scaler_weather0 = StandardScaler()\n",
    "array_weather0 = scaler_weather0.fit_transform(array_weather0)\n",
    "scaler_weather2 = StandardScaler()\n",
    "array_weather2 = scaler_weather2.fit_transform(array_weather2)\n",
    "scaler_weather3 = StandardScaler()\n",
    "array_weather3 = scaler_weather3.fit_transform(array_weather3)\n",
    "#scaler_phase24h = StandardScaler()\n",
    "#array_phase24h = scaler_phase24h.fit_transform(array_phase24h) \n",
    "\n",
    "array_weather = np.empty((dataframe_full.shape[0], array_weather0.shape[1] + array_weather1.shape[1] + array_weather2.shape[1] + array_weather3.shape[1] + 2))\n",
    "\n",
    "array_weather[:,0:array_weather0.shape[1]] = array_weather0\n",
    "array_weather[:,array_weather0.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]] = array_weather1\n",
    "array_weather[:,array_weather0.shape[1]+array_weather1.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]] = array_weather2\n",
    "array_weather[:,array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]+array_weather3.shape[1]] = array_weather3\n",
    "array_weather[:,array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]+array_weather3.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]+array_weather3.shape[1]+2] = array_day_night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis of Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the column headings for the weather data to arrive at \"explained\" weather clusters, which help us acquire some intuition on what different classes of weather data we have acquired.\n",
    "\n",
    "Using the Elbow Method, we determine that we need four classes for k-Means Classification of the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('temp(C)', 28.358161753357237), ('temp_max(C)', 30.092066676956406), ('temp_min(C)', 26.536656891495497), ('humidity(%)', 64.912872356845256), ('pressure(hPa)', 1011.640762463343)]\n",
      "[('temp(C)', 17.652799069561361), ('temp_max(C)', 20.24767390341167), ('temp_min(C)', 14.713225520602567), ('humidity(%)', 71.591603898981461), ('pressure(hPa)', 1017.552503322995)]\n",
      "[('temp(C)', 19.413540181997501), ('temp_max(C)', 21.428023243065567), ('temp_min(C)', 17.47703102729978), ('humidity(%)', 72.516719657932867), ('pressure(hPa)', 1013.1904396447758)]\n",
      "[('temp(C)', 23.917136056364882), ('temp_max(C)', 25.436637151290117), ('temp_min(C)', 22.161096829477309), ('humidity(%)', 47.22088926973268), ('pressure(hPa)', 1015.8051033038178)]\n",
      "[('Clouds', 0.42483407933322814), ('Clear', 0.29587899367188686), ('Rain', 0.11236301898439166), ('Mist', 0.0603488192622329), ('Thunderstorm', 0.060040129649647056), ('Haze', 0.03897206359006563), ('Drizzle', 0.0075628955085656176), ('Fog', 6.6570013390609972e-17)]\n",
      "[('Clear', 0.63181214000880948), ('Clouds', 0.23704031900751887), ('Rain', 0.054607886575096361), ('Mist', 0.051063358440408574), ('Drizzle', 0.012627381479839987), ('Haze', 0.0057598582188803787), ('Fog', 0.0056490917146652985), ('Thunderstorm', 0.0014399645547154734)]\n",
      "[('Clouds', 0.41322223440413708), ('Haze', 0.25249424405212034), ('Mist', 0.21565617805065346), ('Clear', 0.078938712860399995), ('Rain', 0.036838066001531999), ('Thunderstorm', 0.0026312904286784067), ('Drizzle', 0.00021927420238958967), ('Fog', 6.4184768611141862e-17)]\n",
      "[('Clouds', 0.62715414643433376), ('Clear', 0.33028658478531381), ('Rain', 0.021041607159868025), ('Haze', 0.0086641911834782134), ('Thunderstorm', 0.0059982862039382051), ('Mist', 0.0041892792535477008), ('Drizzle', 0.0026659049795291208), ('Fog', 6.5268970783627367e-17)]\n",
      "[('sunrise(h)', 8.2226783968729347), ('sunset(h)', 21.231486340483009), ('wind(m/s)', 3.1636834388022761)]\n",
      "[('sunrise(h)', 6.7295894254917332), ('sunset(h)', 20.08421577314267), ('wind(m/s)', 2.7532853345148331)]\n",
      "[('sunrise(h)', 9.327542667106151), ('sunset(h)', 22.581365347364482), ('wind(m/s)', 2.6494967657054977)]\n",
      "[('sunrise(h)', 6.6556142689400941), ('sunset(h)', 20.001413881738397), ('wind(m/s)', 3.2673674188326993)]\n",
      "[('cloudiness(%)', 39.769563204197603)]\n",
      "[('cloudiness(%)', 18.831745680103875)]\n",
      "[('cloudiness(%)', 64.392829733579902)]\n",
      "[('cloudiness(%)', 34.984099781014478)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "=================================================\n",
    "\tCluster Analysis of Weather Data\n",
    "=================================================\n",
    "'''\n",
    "\n",
    "kMeans_elbow_method_plot(array_weather, 20, getcwd()+\"/data/Weather_kMeans_ElbowMethod.svg\")\n",
    "\n",
    "kBest_weather = 4\n",
    "\n",
    "kMeans_weather = kMeans_model(array_weather, kBest_weather)\n",
    "array_weather_cen0 = scaler_weather0.inverse_transform(kMeans_weather[2][:,0:array_weather0.shape[1]])\n",
    "array_weather_cen1 = kMeans_weather[2][:,array_weather0.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]]\n",
    "array_weather_cen2 = scaler_weather2.inverse_transform(kMeans_weather[2][:,array_weather0.shape[1]+array_weather1.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]])\n",
    "array_weather_cen3 = scaler_weather3.inverse_transform(kMeans_weather[2][:,array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]:array_weather0.shape[1]+array_weather1.shape[1]+array_weather2.shape[1]+array_weather3.shape[1]])\n",
    "\n",
    "list_expl_weather0 = []\n",
    "for cen in array_weather_cen0:\n",
    "\tweather0_expl = [ (list_headers_weather0[i_w0], cen[i_w0]) for i_w0 in range(len(list_headers_weather0)) ]\n",
    "\tprint weather0_expl\n",
    "\tlist_expl_weather0.append(weather0_expl)\n",
    "\n",
    "list_expl_weather1 = []\n",
    "for cen in array_weather_cen1:\n",
    "\tweather1_expl = sorted([ (desc, cen[dict_weather1[desc]]) for desc in dict_weather1.keys() ], key=lambda x: x[1], reverse=True)\n",
    "\tprint weather1_expl\n",
    "\tlist_expl_weather1.append(weather1_expl)\n",
    "\n",
    "list_expl_weather2 = []\n",
    "for cen in array_weather_cen2:\n",
    "\tweather2_expl = [ (list_headers_weather2[i_w2], cen[i_w2]) for i_w2 in range(len(list_headers_weather2)) ]\n",
    "\tprint weather2_expl\n",
    "\tlist_expl_weather2.append(weather2_expl)\n",
    "\n",
    "list_expl_weather3 = []\n",
    "for cen in array_weather_cen3:\n",
    "\tweather3_expl = [ (list_headers_weather3[i_w3], cen[i_w3]) for i_w3 in range(len(list_headers_weather3)) ]\n",
    "\tprint weather3_expl\n",
    "\tlist_expl_weather3.append(weather3_expl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of Weather Data Clusters Based on Heading-Data Key-Value Pairs\n",
    "\n",
    "Well, the text of the output above looks really messy. Let us see what we can derive out of that by deconstructing this manually...\n",
    "\n",
    "---\n",
    "\n",
    "[('temp(C)', 28.358161753357237), ('temp_max(C)', 30.092066676956406), ('temp_min(C)', 26.536656891495497), ('humidity(%)', 64.912872356845256), ('pressure(hPa)', 1011.640762463343)]      \n",
    "\n",
    "    ^ Hot & moderately humid ^\n",
    "\n",
    "[('temp(C)', 17.652799069561361), ('temp_max(C)', 20.24767390341167), ('temp_min(C)', 14.713225520602567), ('humidity(%)', 71.591603898981461), ('pressure(hPa)', 1017.552503322995)]      \n",
    "    \n",
    "    ^ Cool & humid ^\n",
    "    \n",
    "[('temp(C)', 19.413540181997501), ('temp_max(C)', 21.428023243065567), ('temp_min(C)', 17.47703102729978), ('humidity(%)', 72.516719657932867), ('pressure(hPa)', 1013.1904396447758)]     \n",
    "    \n",
    "    ^ Cool & humid ^\n",
    "\n",
    "[('temp(C)', 23.917136056364882), ('temp_max(C)', 25.436637151290117), ('temp_min(C)', 22.161096829477309), ('humidity(%)', 47.22088926973268), ('pressure(hPa)', 1015.8051033038178)]      \n",
    "\n",
    "    ^ Temperate & somewhat humid ^\n",
    "\n",
    "---\n",
    "\n",
    "[('Clouds', 0.42483407933322814), ('Clear', 0.29587899367188686), ('Rain', 0.11236301898439166), ('Mist', 0.0603488192622329), ('Thunderstorm', 0.060040129649647056), ('Haze', 0.03897206359006563), ('Drizzle', 0.0075628955085656176), ('Fog', 6.6570013390609972e-17)]              \n",
    "\n",
    "    ^ Cloudy mixed with clear; some rain ^\n",
    "    \n",
    "[('Clear', 0.63181214000880948), ('Clouds', 0.23704031900751887), ('Rain', 0.054607886575096361), ('Mist', 0.051063358440408574), ('Drizzle', 0.012627381479839987), ('Haze', 0.0057598582188803787), ('Fog', 0.0056490917146652985), ('Thunderstorm', 0.0014399645547154734)]      \n",
    "\n",
    "    ^ Clear with some clouds ^\n",
    "\n",
    "[('Clouds', 0.41322223440413708), ('Haze', 0.25249424405212034), ('Mist', 0.21565617805065346), ('Clear', 0.078938712860399995), ('Rain', 0.036838066001531999), ('Thunderstorm', 0.0026312904286784067), ('Drizzle', 0.00021927420238958967), ('Fog', 6.4184768611141862e-17)]             \n",
    "\n",
    "    ^ Cloudy and hazy/misty ^\n",
    "\n",
    "[('Clouds', 0.62715414643433376), ('Clear', 0.33028658478531381), ('Rain', 0.021041607159868025), ('Haze', 0.0086641911834782134), ('Thunderstorm', 0.0059982862039382051), ('Mist', 0.0041892792535477008), ('Drizzle', 0.0026659049795291208), ('Fog', 6.5268970783627367e-17)]              \n",
    "\n",
    "    ^ Cloudy mixed with clear; almost no rain ^\n",
    "\n",
    "---\n",
    "\n",
    "[('sunrise(h)', 8.2226783968729347), ('sunset(h)', 21.231486340483009), ('wind(m/s)', 3.1636834388022761)]\n",
    "[('sunrise(h)', 6.7295894254917332), ('sunset(h)', 20.08421577314267), ('wind(m/s)', 2.7532853345148331)]\n",
    "[('sunrise(h)', 9.327542667106151), ('sunset(h)', 22.581365347364482), ('wind(m/s)', 2.6494967657054977)]\n",
    "[('sunrise(h)', 6.6556142689400941), ('sunset(h)', 20.001413881738397), ('wind(m/s)', 3.2673674188326993)]\n",
    "\n",
    "---\n",
    "\n",
    "[('cloudiness(%)', 39.769563204197603)]       <--- Mostly clear with clouds\n",
    "\n",
    "[('cloudiness(%)', 18.831745680103875)]       <--- Mostly clear\n",
    "\n",
    "[('cloudiness(%)', 64.392829733579902)]       <--- Mostly cloudy with some clearness\n",
    "\n",
    "[('cloudiness(%)', 34.984099781014478)]       <--- Mostly clear with clouds\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the manual observations below, we clasify weather data into the four following categories:\n",
    "    * Type 0: hot, quite humid, mix of clear & cloudy; some rain\n",
    "    * Type 1: cool, humid, mostly clear\n",
    "    * Type 2: cool, humid, cloudy/misty/hazy\n",
    "    * Type 3: temperate, slightly humid, mix of clear & cloudy, ~0 rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method Plot for Optimal k-Means Clustering of Weather Data\n",
    "\n",
    "Here is the Elbow Method plot justifying the four-fold classification of weather types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/Weather_kMeans_ElbowMethod.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Sentiments and Circadian Rhythm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to start synthesising the tweet data with the weather data. We start off by looking at the journal phase and the tweet sentiments. \n",
    "\n",
    "We note that the Twitter dataset was gathered between August 24th and 25th, 2017, both of them weekdays. \n",
    "\n",
    "As a preliminary form of analysis, let us plot mean and standard deviations of sentiment scores as functions of the journal phase. We use a 100-point sample of the journal phase, each point effectively corresponding to a little less than fifteen minutes of Twitter data (24 hours in the day divided into 100 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "================================================================\n",
    "\tAnalysis: Tweet Sentiments vs. Circadian Rhythm\n",
    "================================================================\n",
    "'''\n",
    "\n",
    "num_divs_phase24h = 100\n",
    "delta_phase24h = 2*np.pi/num_divs_phase24h\n",
    "array_index_phase24h = np.array(np.ceil((array_phase24h[:,0] + np.pi)/delta_phase24h)-1, dtype=int)\n",
    "\n",
    "nested_list_AFINN = []\n",
    "nested_list_comp = []\n",
    "nested_list_neg = []\n",
    "nested_list_neu = []\n",
    "nested_list_pos = []\n",
    "\n",
    "array_AFINN = array_sentiments[:,0]\n",
    "array_comp = array_sentiments[:,1]\n",
    "array_neg = array_sentiments[:,2]\n",
    "array_neu = array_sentiments[:,3]\n",
    "array_pos = array_sentiments[:,4]                                            \n",
    "\n",
    "for i_phase24h in range(num_divs_phase24h):                                         \n",
    "\tnested_list_AFINN.append([])             \n",
    "\tnested_list_comp.append([])                              \n",
    "\tnested_list_neg.append([])                             \n",
    "\tnested_list_neu.append([])                           \n",
    "\tnested_list_pos.append([]) \n",
    "\n",
    "for i_data in range(dataframe_full.values.shape[0]):                                \n",
    "\ti_phase24h = array_index_phase24h[i_data]\n",
    "\tnested_list_AFINN[i_phase24h].append(array_AFINN[i_data])\n",
    "\tnested_list_comp[i_phase24h].append(array_comp[i_data])\n",
    "\tnested_list_neg[i_phase24h].append(array_neg[i_data])\n",
    "\tnested_list_neu[i_phase24h].append(array_neu[i_data])\n",
    "\tnested_list_pos[i_phase24h].append(array_pos[i_data])\n",
    "\n",
    "array_sample_phase24h = np.arange(-np.pi+0.5*delta_phase24h, +np.pi, delta_phase24h)\n",
    "\n",
    "list_avg_AFINN = []\n",
    "list_avg_comp = []\n",
    "list_avg_neg = []\n",
    "list_avg_neu = []\n",
    "list_avg_pos = []\n",
    "\n",
    "list_stdev_AFINN = []\n",
    "list_stdev_comp = []\n",
    "list_stdev_neg = []\n",
    "list_stdev_neu = []\n",
    "list_stdev_pos = []\n",
    "\n",
    "for i_phase24h in range(num_divs_phase24h):\n",
    "\tlist_avg_AFINN.append(np.mean(nested_list_AFINN[i_phase24h]))\n",
    "\tlist_avg_comp.append(np.mean(nested_list_comp[i_phase24h]))\n",
    "\tlist_avg_neg.append(np.mean(nested_list_neg[i_phase24h]))\n",
    "\tlist_avg_neu.append(np.mean(nested_list_neu[i_phase24h]))\n",
    "\tlist_avg_pos.append(np.mean(nested_list_pos[i_phase24h]))\n",
    "\tlist_stdev_AFINN.append(np.std(nested_list_AFINN[i_phase24h]))\n",
    "\tlist_stdev_comp.append(np.std(nested_list_comp[i_phase24h]))\n",
    "\tlist_stdev_neg.append(np.std(nested_list_neg[i_phase24h]))\n",
    "\tlist_stdev_neu.append(np.std(nested_list_neu[i_phase24h]))\n",
    "\tlist_stdev_pos.append(np.std(nested_list_pos[i_phase24h]))\n",
    "\t\n",
    "phase24h_avg_fgr = plt.figure()\n",
    "phase24h_avg_fgr.clf()\n",
    "phase24h_avg_plt = phase24h_avg_fgr.add_subplot(1,1,1)\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, np.array(list_avg_AFINN), linestyle='-', marker='o', color='c', label=\"AFINN\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, np.array(list_avg_comp), linestyle='-', marker='o', color='g', label=\"comp\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, np.array(list_avg_neg), linestyle='-', marker='o', color='b', label=\"neg\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, np.array(list_avg_neu), linestyle='-', marker='o', color='k', label=\"neu\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, np.array(list_avg_pos), linestyle='-', marker='o', color='y', label=\"pos\")\n",
    "phase24h_avg_plt.set_xlabel(\"Journal Phase, $\\delta$ [radians]\")\n",
    "phase24h_avg_plt.set_ylabel(\"Average Sentiment Score, $\\overline{\\Sigma}$ [dimensionless]\")\n",
    "phase24h_avg_plt.legend()\n",
    "phase24h_avg_fgr.savefig(getcwd()+\"/data/AverageSentiment_CircadianRhythm.svg\")\n",
    "\n",
    "phase24h_stdev_fgr = plt.figure()\n",
    "phase24h_stdev_fgr.clf()\n",
    "phase24h_stdev_plt = phase24h_stdev_fgr.add_subplot(1,1,1)\n",
    "phase24h_stdev_plt.plot(array_sample_phase24h, np.array(list_stdev_AFINN), linestyle='-', marker='o', color='c', label=\"AFINN\")\n",
    "phase24h_stdev_plt.plot(array_sample_phase24h, np.array(list_stdev_comp), linestyle='-', marker='o', color='g', label=\"comp\")\n",
    "phase24h_stdev_plt.plot(array_sample_phase24h, np.array(list_stdev_neg), linestyle='-', marker='o', color='b', label=\"neg\")\n",
    "phase24h_stdev_plt.plot(array_sample_phase24h, np.array(list_stdev_neu), linestyle='-', marker='o', color='k', label=\"neu\")\n",
    "phase24h_stdev_plt.plot(array_sample_phase24h, np.array(list_stdev_pos), linestyle='-', marker='o', color='y', label=\"pos\")\n",
    "phase24h_stdev_plt.set_xlabel(\"Journal Phase, $\\delta$ [radians]\")\n",
    "phase24h_stdev_plt.set_ylabel(\"Standard Deviation of Sentiment Score, $\\sigma_{\\Sigma}$ [dimensionless]\")\n",
    "phase24h_stdev_plt.legend()\n",
    "phase24h_stdev_fgr.savefig(getcwd()+\"/data/StandardDeviationSentiment_CircadianRhythm.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did the results turn out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Data for Mean Sentiment Scores (Over Respective Sampling Time-Period) vs. Journal Phase\n",
    "\n",
    "![title](data/AverageSentiment_CircadianRhythm.svg)\n",
    "\n",
    "#### Raw Data for Standard Deviaition of Sentiment Scores (Over Respective Sampling Time-Period) vs. Journal Phase\n",
    "\n",
    "![title](data/StandardDeviationSentiment_CircadianRhythm.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes: \n",
    "    * The AFINN plot is based on the AFINN sentiment score.\n",
    "    * The other plots are NLTK sentiment scores: compound (comp), negative (neg), neutral (neu), and positive (pos).\n",
    "\n",
    "The data seems really noisy. =(\n",
    "\n",
    "Let us try a smoothing filter (nothing fancy: just a nearest-neighbour averaging scheme) to reduce some of the noise from the average sentiment score. [We could also try to use a smaller number of samples for the journal phase, $\\delta$.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_smoothing(input_list, window_size=3):\n",
    "\toutput_list = []\n",
    "\tfor i_data in range(len(input_list)):\n",
    "\t\twindow_array = np.array(range(i_data-window_size, i_data+window_size+1), dtype=int) % len(input_list)\n",
    "\t\tsum_window = 0.0\n",
    "\t\tfor i_window in window_array:\n",
    "\t\t\tsum_window += input_list[i_window]\n",
    "\t\toutput_list.append( sum_window / (2*window_size + 1) )\n",
    "\treturn np.array(output_list, dtype=float)\n",
    "\n",
    "phase24h_avg_fgr = plt.figure()\n",
    "phase24h_avg_fgr.clf()\n",
    "phase24h_avg_plt = phase24h_avg_fgr.add_subplot(1,1,1)\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_AFINN), linestyle='-', marker='o', color='c', label=\"AFINN\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_comp), linestyle='-', marker='o', color='g', label=\"comp\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_neg), linestyle='-', marker='o', color='b', label=\"neg\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_neu), linestyle='-', marker='o', color='k', label=\"neu\")\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_pos), linestyle='-', marker='o', color='y', label=\"pos\")\n",
    "phase24h_avg_plt.set_xlabel(\"Journal Phase, $\\delta$ [radians]\")\n",
    "phase24h_avg_plt.set_ylabel(\"Smoothed Average Sentiment Score, $\\overline{\\Sigma}$ [dimensionless]\")\n",
    "phase24h_avg_plt.legend()\n",
    "phase24h_avg_fgr.savefig(getcwd()+\"/data/AverageSentimentSmoothed_CircadianRhythm.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we fare? \n",
    "\n",
    "#### Smoothed Data for Mean Sentiment Scores (Over Respective Sampling Time-Period) vs. Journal Phase\n",
    "\n",
    "![title](data/AverageSentimentSmoothed_CircadianRhythm.svg)\n",
    "\n",
    "This seems slightly better. We seem to be seeing a decent progression for the AFINN data, but we are not seeing much from the NLTK scores. Let us plot the AFINN and NLTK compound scores separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase24h_avg_fgr = plt.figure()\n",
    "phase24h_avg_fgr.clf()\n",
    "phase24h_avg_plt = phase24h_avg_fgr.add_subplot(1,1,1)\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_AFINN), linestyle='-', marker='o', color='c', label=\"AFINN\")\n",
    "phase24h_avg_plt.set_xlabel(\"Journal Phase, $\\delta$ [radians]\")\n",
    "phase24h_avg_plt.set_ylabel(\"Smoothed Average Sentiment Score, $\\overline{\\Sigma}$ [dimensionless]\")\n",
    "phase24h_avg_plt.legend()\n",
    "phase24h_avg_fgr.savefig(getcwd()+\"/data/AverageSentimentSmoothed_CircadianRhythm_AFINN.svg\")\n",
    "\n",
    "phase24h_avg_fgr = plt.figure()\n",
    "phase24h_avg_fgr.clf()\n",
    "phase24h_avg_plt = phase24h_avg_fgr.add_subplot(1,1,1)\n",
    "phase24h_avg_plt.plot(array_sample_phase24h, average_smoothing(list_avg_comp), linestyle='-', marker='o', color='g', label=\"comp\")\n",
    "phase24h_avg_plt.set_xlabel(\"Journal Phase, $\\delta$ [radians]\")\n",
    "phase24h_avg_plt.set_ylabel(\"Smoothed Average Sentiment Score, $\\overline{\\Sigma}$ [dimensionless]\")\n",
    "phase24h_avg_plt.legend()\n",
    "phase24h_avg_fgr.savefig(getcwd()+\"/data/AverageSentimentSmoothed_CircadianRhythm_compNLTK.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothed Data for Mean AFINN Sentiment Scores (Over Respective Sampling Time-Period) vs. Journal Phase\n",
    "\n",
    "![title](data/AverageSentimentSmoothed_CircadianRhythm_AFINN.svg)\n",
    "\n",
    "#### Smoothed Data for Mean Compound NLTK Sentiment Scores (Over Respective Sampling Time-Period) vs. Journal Phase\n",
    "\n",
    "![title](data/AverageSentimentSmoothed_CircadianRhythm_compNLTK.svg)\n",
    "\n",
    "\n",
    "It is reassuring to see that the NLTK compoud scores follow the AFINN score trends. =)\n",
    "\n",
    "Let us take a closer look at the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mood-Changing Effects of Sunrise, Sunset, and Midnight\n",
    "\n",
    "For $\\delta\\,\\in\\,\\left(-\\pi,\\,-\\frac{2}{3}\\pi\\right)$, there seems to be a spike in the tweet sentiment positivity. This is probabbly due to the after-work relaxation tweeting on a weekday before bedtime. \n",
    "\n",
    "For $\\delta\\,\\approx\\,-\\frac{1}{2}\\pi$, the lowest point of the tweet sentiment scores occur. We are tempted to ask: Are people more likely to be depressed in the middle of the night?\n",
    "\n",
    "This is followed by a rise in positivity, then by annother slump.\n",
    "\n",
    "We also notice a rise in positivity of the tweet scores for $\\delta\\,\\approx\\,0$, i.e. near sunrise. This could be of interest from a circadian rhythm perspective. \n",
    "\n",
    "The tweet positivity levels fall as we progress further into the day, to pick up later (perhaps, after lunch-break and in anticipation of the end of the workday?). \n",
    "\n",
    "There seems to be a rise is positivity level, perhaps, at the end of the workday for most people.\n",
    "\n",
    "Lastly, we note that tweet sentiments are at a low at sunset, i.e. for $\\delta\\,\\approx\\,\\pm\\pi$.\n",
    "\n",
    "The data seems to suggest that circadian rhythm may play an important role in the sentiment of Twitter data text. Sunrise seems to be a time of positive sentiments, whereas sunset and the middle of the night seem to be times of negativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Principal Component Analysis on the Weather Data\n",
    "\n",
    "There seems to be a very large set of weather data parameters, but it stands to reason that not all of these would be relevant. This is a good time to try a Principal Component Analysis to determine what the key players are in shaping of the weather of the United States for the period under scrutiny.\n",
    "\n",
    "Using our old technique of merging headers and data components, we will also try to come up with some explanation of what is the most significant amongst the 19 weather data parameters acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('temp(C)', 0.5180331167630976), ('temp_max(C)', 0.51152783034572524), ('temp_min(C)', 0.50753338754022204), ('pressure(hPa)', -0.34315047073388766), ('humidity(%)', -0.23400017487206859), ('day(_)', 0.096083256417524229), ('night(_)', -0.096068499777244132), ('sunrise(h)', 0.08981705761092601), ('wind(m/s)', 0.088185697662805054), ('Clouds', 0.044348705784236116), ('sunset(h)', 0.036816038178388799), ('Clear', -0.034478424033643512), ('Mist', -0.02262809374845005), ('cloudiness(%)', 0.018393007915670841), ('Thunderstorm', 0.010034662349665865), ('Rain', 0.009191869494754595), ('Haze', -0.0054419375758603273), ('Fog', -0.0011773642586568572), ('Drizzle', 0.00015058198795230422)], [('sunrise(h)', 0.59352838441785971), ('sunset(h)', 0.58111428140381938), ('cloudiness(%)', 0.3295353838940176), ('humidity(%)', 0.29539681519763117), ('pressure(hPa)', -0.26597435073218273), ('Clear', -0.12555515318540653), ('wind(m/s)', -0.092434749373820596), ('temp(C)', -0.068330263721813833), ('temp_max(C)', -0.066143004169692599), ('Haze', 0.062164203545584745), ('Mist', 0.054635085893173084), ('temp_min(C)', -0.047861324099155826), ('night(_)', 0.021001059972513454), ('day(_)', -0.020989590309143491), ('Rain', 0.0070297485033121573), ('Thunderstorm', 0.003198731719676621), ('Clouds', -0.0012105558479668273), ('Drizzle', -0.00015327253199513187), ('Fog', -0.00010878809637566078)], [('wind(m/s)', 0.65453314587334477), ('cloudiness(%)', 0.53415043208685309), ('pressure(hPa)', 0.29458711674660698), ('Clouds', 0.27413836601709879), ('Clear', -0.25823178574261541), ('humidity(%)', -0.14604680920506746), ('night(_)', -0.11984244764480202), ('day(_)', 0.11983495039391248), ('temp_max(C)', -0.055291715647311952), ('Haze', -0.042897337906505642), ('sunrise(h)', -0.036997993072330225), ('Rain', 0.025656543952866588), ('temp(C)', -0.022680156685648541), ('Mist', -0.0090216298983877598), ('Thunderstorm', 0.0067303112482922168), ('temp_min(C)', -0.0039555731118033899), ('Drizzle', 0.0034438376517718573), ('sunset(h)', -0.0026887522952316278), ('Fog', 0.00018169467746661613)], [('cloudiness(%)', -0.53947147615509561), ('humidity(%)', -0.49868068946454436), ('sunset(h)', 0.39495756948710287), ('wind(m/s)', 0.30916677110913543), ('sunrise(h)', 0.30859833277213011), ('pressure(hPa)', 0.17738390420501909), ('Clear', 0.1562753306054592), ('night(_)', -0.11572145736722911), ('day(_)', 0.11569607789186835), ('temp_min(C)', -0.10688693874461214), ('temp(C)', -0.081589809814246517), ('Clouds', -0.070231182141927084), ('temp_max(C)', -0.065061252147845722), ('Mist', -0.045020281854573345), ('Rain', -0.038529584554341113), ('Drizzle', -0.0055913409808678385), ('Haze', 0.0047119898114683237), ('Fog', -0.0018798326378919434), ('Thunderstorm', 0.00026490175266854522)], [('wind(m/s)', 0.66126297057474259), ('humidity(%)', 0.53858665753505441), ('cloudiness(%)', -0.29471139439719501), ('pressure(hPa)', -0.27198932186246627), ('day(_)', -0.16037598460689839), ('night(_)', 0.16034245431032193), ('Clouds', -0.15454997233876411), ('Clear', 0.13635249031836216), ('sunset(h)', -0.078314707164119035), ('temp_max(C)', 0.07166932317769685), ('Haze', -0.055409026713700342), ('Mist', 0.033258939066573227), ('temp_min(C)', -0.033241439517280126), ('Thunderstorm', 0.024223857309980584), ('sunrise(h)', -0.020114043428410083), ('temp(C)', 0.013817555949675096), ('Rain', 0.013305485311196363), ('Drizzle', 0.0019318014562445184), ('Fog', 0.00088642559010230181)]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "=========================================================\n",
    "\tWeather Data Principal Component Analysis\n",
    "=========================================================\n",
    "'''\n",
    "\n",
    "pca_elbow_method_plot(array_weather, 19, getcwd()+\"/data/PCA_WeatherData.svg\")\n",
    "weather_components_PCA = pca_model(array_weather, 5)[1] \n",
    "list_headers_weather = list_headers_weather0 + list(dict_weather1.keys()) + list_headers_weather2 + list_headers_weather3 + list_headers_day_night\n",
    "\n",
    "weather_components_explained = []\n",
    "for i_component in range(weather_components_PCA.shape[0]):\n",
    "\tcomponent_vector = weather_components_PCA[i_component]\n",
    "\texplained_component_raw = sorted([(list_headers_weather[i_basis], np.abs(component_vector[i_basis]), np.abs(component_vector[i_basis])/component_vector[i_basis]) for i_basis in range(weather_components_PCA.shape[1])], key=lambda x: x[1], reverse=True)\n",
    "\texplained_component = [(explained_component_raw[i_tuple][0], explained_component_raw[i_tuple][1]*explained_component_raw[i_tuple][2]) for i_tuple in range(len(explained_component_raw))]\n",
    "\tweather_components_explained.append(explained_component)\n",
    "print weather_components_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of Principal Components of Weather Based on Header-Data Key-Value Pairings\n",
    "\n",
    "Let us try to sift through the mess of the output text by adding some extra whitespace at the necessary places...\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "[\n",
    "[('temp(C)', 0.51803311676311825), ('temp_max(C)', 0.51152783034569516), ('temp_min(C)', 0.50753338754022181), ('pressure(hPa)', -0.34315047073390426), ('humidity(%)', -0.23400017487209315), ('day(_)', 0.096083256417510227), ('night(_)', -0.096068499777248698), ('sunrise(h)', 0.089817057610875786), ('wind(m/s)', 0.08818569766281921), ('Clouds', 0.044348705784233361), ('sunset(h)', 0.03681603817834616), ('Clear', -0.034478424033634401), ('Mist', -0.022628093748451261), ('cloudiness(%)', 0.018393007915639192), ('Thunderstorm', 0.010034662349665709), ('Rain', 0.0091918694947534917), ('Haze', -0.0054419375758630942), ('Fog', -0.0011773642586568346), ('Drizzle', 0.00015058198795306671)],      \n",
    "\n",
    "    ^ \"Perceived temperature\" (humidex, wind-chill, etc) ^\n",
    "\n",
    "[('sunrise(h)', 0.5935283844178727), ('sunset(h)', 0.5811142814037783), ('cloudiness(%)', 0.32953538389399872), ('humidity(%)', 0.29539681519764799), ('pressure(hPa)', -0.26597435073225795), ('Clear', -0.12555515318540794), ('wind(m/s)', -0.092434749373840247), ('temp(C)', -0.068330263721767925), ('temp_max(C)', -0.066143004169659458), ('Haze', 0.062164203545594335), ('Mist', 0.054635085893178149), ('temp_min(C)', -0.047861324099138611), ('night(_)', 0.021001059972511123), ('day(_)', -0.02098959030914066), ('Rain', 0.0070297485033139389), ('Thunderstorm', 0.0031987317196776658), ('Clouds', -0.0012105558479702135), ('Drizzle', -0.00015327253200441893), ('Fog', -0.00010878809637685213)],\t   \n",
    "\n",
    "    ^ \"Light exposure & air/water interaction\" axis ^\n",
    "\n",
    "[('wind(m/s)', 0.65453314587331901), ('cloudiness(%)', 0.53415043208683632), ('pressure(hPa)', 0.29458711674661014), ('Clouds', 0.2741383660171155), ('Clear', -0.25823178574262967), ('humidity(%)', -0.14604680920516366), ('night(_)', -0.1198424476448092), ('day(_)', 0.11983495039391612), ('temp_max(C)', -0.055291715647165555), ('Haze', -0.042897337906536985), ('sunrise(h)', -0.03699799307240996), ('Rain', 0.025656543952825725), ('temp(C)', -0.022680156685964167), ('Mist', -0.0090216298984092669), ('Thunderstorm', 0.0067303112482915862), ('temp_min(C)', -0.0039555731116531767), ('Drizzle', 0.0034438376518846491), ('sunset(h)', -0.0026887522950110277), ('Fog', 0.00018169467747615602)],\t  \n",
    "\n",
    "    ^ \"Cloud formation\" axis 1 (wind and pressure) ^\n",
    "\n",
    "[('cloudiness(%)', -0.53947147615500024), ('humidity(%)', -0.49868068946454747), ('sunset(h)', 0.39495756948909522), ('wind(m/s)', 0.30916677110888174), ('sunrise(h)', 0.30859833277013315), ('pressure(hPa)', 0.17738390420498362), ('Clear', 0.15627533060536469), ('night(_)', -0.11572145736716272), ('day(_)', 0.11569607789180718), ('temp_min(C)', -0.1068869387438596), ('temp(C)', -0.081589809815525896), ('Clouds', -0.070231182142093063), ('temp_max(C)', -0.065061252147040297), ('Mist', -0.045020281854660435), ('Rain', -0.038529584554613291), ('Drizzle', -0.0055913409802464059), ('Haze', 0.0047119898113903366), ('Fog', -0.0018798326377839857), ('Thunderstorm', 0.00026490175263674731)],\t     \n",
    "\n",
    "    ^ \"Cloud formation\" axis 2 (humidity, wind, and light) ^\n",
    "\n",
    "[('wind(m/s)', 0.66126297057486971), ('humidity(%)', 0.5385866575350049), ('cloudiness(%)', -0.29471139439723354), ('pressure(hPa)', -0.27198932186246566), ('day(_)', -0.16037598460692196), ('night(_)', 0.16034245431034738), ('Clouds', -0.15454997233896151), ('Clear', 0.13635249031825808), ('sunset(h)', -0.07831470716211425), ('temp_max(C)', 0.071669323178983987), ('Haze', -0.055409026713919965), ('Mist', 0.033258939066420003), ('temp_min(C)', -0.033241439516068047), ('Thunderstorm', 0.024223857309966325), ('sunrise(h)', -0.020114043430359701), ('temp(C)', 0.013817555947356177), ('Rain', 0.013305485310842698), ('Drizzle', 0.001931801457181), ('Fog', 0.00088642559023209761)] \t\n",
    "\n",
    "    ^ \"Cloud formation\" axis 3 (wind and humidity) ^\n",
    "\n",
    "---\n",
    "\n",
    "There seem to be five principal components of weather data based on an 85% cut-off for the explained variance in PCA:\n",
    "    1. Perceived temperature: temperature and humidity\n",
    "    2. Sunlight and air/water interaction: humidity, pressure, etc.\n",
    "    3. Cloud formation:\n",
    "        a. Wind and pressure\n",
    "        b. Humidity, wind, and sunlight\n",
    "        c. Wind and humidity\n",
    "        \n",
    "Lastly, we provide a plot for the explained variances for the PCA.\n",
    "\n",
    "#### Explained Variance Plot for PCA of Weather Data\n",
    "\n",
    "![title](data/PCA_WeatherData.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical Distribution of Weather and Negative-Sentiment Tweets\n",
    "\n",
    "We propose to look at the proportion of negative-sentiment tweets as a consequence of geographical location and weather type. We tally up the negative-sentiment tweets for the various weather types identfied above for every US city studied. We only consider the location and weather combination for which at least 10 tweets have been recorded. \n",
    "\n",
    "The proportion of negative-sentiment tweets in a sample is represented by the colour of the scatter plot. The shape of the marker represents the regional classification (resultant from k-Means Clustering of location coordinates) and the size of the marker is representative of the sample-size of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "=========================================================================================================\n",
    "\tScatter Plots of Negative-Sentiment Tweets -- Georgraphical Distributions by Weather Type\n",
    "=========================================================================================================\n",
    "'''\n",
    "\n",
    "# Dictionary Structure with City Data: Key = City Name, Value = [State, (Latitude, Longitude), UTC_offset, OpenWeatherMap_city_code]\n",
    "dict_city_coordinate =\t{\"new york\": (40.6643, -73.9385),\n",
    "\t\t\t\"los angeles\": (34.0194, -118.4108),\n",
    "\t\t\t\"chicago\": (41.8376, -87.6818),\n",
    "\t\t\t\"houston\": (29.7805, -95.3863),\n",
    "\t\t\t\"phoenix\": (33.5722, -112.0880),\n",
    "\t\t\t\"philadelphia\": (40.0094, -75.1333),\n",
    "\t\t\t\"san antonio\": (29.4724, -98.5251),\n",
    "\t\t\t\"san diego\": (32.8153, -117.1350),\n",
    "\t\t\t\"dallas\": (32.7757, -96.7967),\n",
    "\t\t\t\"san Jose\": (37.2969, -121.8193),\n",
    "\t\t\t\"austin\": (30.3072, -97.7560),\n",
    "\t\t\t\"jacksonville\": (30.3370, -81.6613),\n",
    "\t\t\t\"san francisco\": (37.7751, -122.4193),\n",
    "\t\t\t\"columbus\": (39.9848, -82.9850),\n",
    "\t\t\t\"indianapolis\": (39.7767, -86.1459),\n",
    "\t\t\t\"fort worth\": (32.7795, -97.3463),\n",
    "\t\t\t\"charlotte\": (35.2087, -80.8307),\n",
    "\t\t\t\"seattle\": (47.6205, -122.3509),\n",
    "\t\t\t\"denver\": (39.7618, -104.8806),\n",
    "\t\t\t\"el paso\": (31.8484, -106.4270),\n",
    "\t\t\t\"washington\": (38.9041, -77.0171),\n",
    "\t\t\t\"boston\": (42.3320, -71.0202),\n",
    "\t\t\t\"detroit\": (42.3830, -83.1022),\n",
    "\t\t\t\"nashville\": (36.1718, -86.7850),\n",
    "\t\t\t\"memphis\": (35.1035, -89.9785),\n",
    "\t\t\t\"portland, or\": (45.5370, -122.6500),\t# Special city key: to differentiate entry from Portland, ME\n",
    "\t\t\t\"oklahoma city\": (35.4671, -97.5137),\n",
    "\t\t\t\"las vegas\": (36.2277, -115.2640),\n",
    "\t\t\t\"louisville\": (38.1781, -85.6667),\n",
    "\t\t\t\"baltimore\": (39.3002, -76.6105)}\n",
    "\n",
    "def find_city_key(coordinates, city_coordinate_dict):\n",
    "\tfor key in city_coordinate_dict.keys():\n",
    "\t\tif (city_coordinate_dict[key][0] == coordinates[0]) and (city_coordinate_dict[key][1] == coordinates[1]):\n",
    "\t\t\treturn key\n",
    "\treturn None\n",
    "\n",
    "# Dictionary of Sentiment Data by City\n",
    "# * City name as key\n",
    "# * For each weather type, counts of negative-sentiment tweets and all tweets to be tallied\n",
    "# * Each city to contain region class\n",
    "dict_city_wthr_sntmt =\t{\"new york\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"los angeles\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"chicago\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"houston\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"phoenix\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"philadelphia\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"san antonio\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"san diego\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"dallas\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"san Jose\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"austin\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"jacksonville\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"san francisco\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"columbus\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"indianapolis\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"fort worth\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"charlotte\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"seattle\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"denver\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"el paso\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"washington\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"boston\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"detroit\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"nashville\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"memphis\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"portland, or\": [np.zeros((kBest_weather, 2), dtype=float), -1],\t# Special city key: to differentiate entry from Portland, ME\n",
    "\t\t\t\"oklahoma city\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"las vegas\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"louisville\": [np.zeros((kBest_weather, 2), dtype=float), -1],\n",
    "\t\t\t\"baltimore\": [np.zeros((kBest_weather, 2), dtype=float), -1]}\n",
    "\n",
    "#array_coordinates_scaled = scaler_coordinates.fit_transform(array_coordinates)\n",
    "#kMeans_coordinate_all_labels = kMeans_model(array_coordinates_scaled, kBest_coordinates)[1]\n",
    "\n",
    "# Updating Region Labels for City-Weather-Sentiment Dictionary\n",
    "for i_coord in range(len(array_coordinates_unique)):\n",
    "\tkey = find_city_key(array_coordinates_unique[i_coord], dict_city_coordinate)\n",
    "\tif key != None:\n",
    "\t\tdict_city_wthr_sntmt[key][1] = array_kMeans_coordinate_labels[i_coord]\t\n",
    "\t\t\t\t\n",
    "kMeans_weather_labels = kMeans_weather[1]\n",
    "#print \"# Diagnostic: kMeans_weather_labels = \"\n",
    "#print kMeans_weather_labels\n",
    "\n",
    "for i_data in range(dataframe_full.values.shape[0]):\n",
    "\tcity_coordinates = array_coordinates[i_data]\n",
    "\tcity_key = find_city_key(city_coordinates, dict_city_coordinate)\n",
    "#\tprint \"# Diagnostic: city_key = \"+str(city_key)\n",
    "\tweather_label = kMeans_weather_labels[i_data]\n",
    "#\tprint \"# Diagnostic: weather_label = \"+str(weather_label)\n",
    "\tif (city_key != None):\n",
    "\t\tdict_city_wthr_sntmt[city_key][0][weather_label][1] += 1\t# Incrementing total tweet count for city + weather-type conbination\n",
    "\t\tif (array_AFINN[i_data] < 0) or (array_comp[i_data] < 0):\n",
    "\t\t\tdict_city_wthr_sntmt[city_key][0][weather_label][0] += 1\t# Incrementing negative-sentimet tweet count for city + weather-type conbination\n",
    "\n",
    "list_plot_markers = ['o', 'D', 's']\n",
    "\n",
    "data_threshold = 10\n",
    "list_neg_prop = []\n",
    "for city_key in dict_city_wthr_sntmt.keys():\n",
    "\tfor i_weather in range(kBest_weather):\n",
    "\t\tif (dict_city_wthr_sntmt[city_key][0][i_weather][1] > data_threshold):\t\t\n",
    "\t\t\tlist_neg_prop.append(dict_city_wthr_sntmt[city_key][0][i_weather][0] / dict_city_wthr_sntmt[city_key][0][i_weather][1])\t\n",
    "min_neg_prop = min(list_neg_prop)\n",
    "max_neg_prop = max(list_neg_prop)\n",
    "\n",
    "for i_weather in range(kBest_weather):\n",
    "\tbg_map = create_background_map()\n",
    "\tfor i_region in range(kBest_coordinates):\n",
    "\t\tfor city_key in dict_city_wthr_sntmt.keys():\n",
    "\t\t\tif (dict_city_wthr_sntmt[city_key][1] == i_region) and (dict_city_wthr_sntmt[city_key][0][i_weather][1] > data_threshold):\n",
    "\t\t\t\tneg_tweet_proportion = dict_city_wthr_sntmt[city_key][0][i_weather][0] / dict_city_wthr_sntmt[city_key][0][i_weather][1]\n",
    "\t\t\t\tx, y = bg_map(dict_city_coordinate[city_key][1], dict_city_coordinate[city_key][0])\n",
    "\t\t\t\tsct_plt0 = bg_map.scatter(x, y, c=neg_tweet_proportion, edgecolors='r', s=200.0+dict_city_wthr_sntmt[city_key][0][i_weather][1], marker=list_plot_markers[i_region], vmin=min_neg_prop, vmax=max_neg_prop, cmap=cm.viridis_r, alpha=1.0)\n",
    "\t\t\t\tsct_plt1 = bg_map.scatter(x, y, color='r', s=20.0, marker='o', alpha=1.0)\n",
    "\tplt.colorbar(sct_plt0)\n",
    "\tplt.savefig(getcwd()+\"/data/WeatherType\"+str(i_weather)+\"_Sentiments.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The West Coast is Happier, the East Coast is Happier by Day, and the South is Unhappy\n",
    "\n",
    "We start off by looking at Type 0 weather:\n",
    "\n",
    "##### Type 0 Weather -- The Hot and Dry South\n",
    "\n",
    "![title](data/WeatherType0_Sentiments.svg)\n",
    "\n",
    "We notice that the hot, dry weather characteristic of Type 0 is seen mostly in cities of the South region (diamond-shaped markers on the plot above). \n",
    "\n",
    "##### Type 1 Weather -- Cool, Humid, and Clear Nights of the East Coast and Mid-West\n",
    "\n",
    "Type 1 weather (cool, humid, mostly clear) is found in the East Coast and Mid-West region (circular markers):\n",
    "\n",
    "![title](data/WeatherType1_Sentiments.svg)\n",
    "\n",
    "##### Type 2 Weather -- Cool and Misty West Coast Ambience\n",
    "\n",
    "Type 2 weather (cool, misty/hazy) is found in the West Coast and Rocky Mountain regions (square markers):\n",
    "\n",
    "![title](data/WeatherType2_Sentiments.svg)\n",
    "\n",
    "It is interesting how the fabled fog in cities like San Francisco have made it to our dataset. =)\n",
    "\n",
    "##### Type 3 Weather -- Temperate, Humid, and Clear Days of the East Coast and Mid-West\n",
    "\n",
    "Lastly, Type 3 (temperate, slightly humid, mostly clear) weather is also characteristic of the East Coast and Mid-West. In fact, it stands to reason that Type 3 is daytime weather for this region, whereas Type 1 is night-time weather.\n",
    "\n",
    "![title](data/WeatherType3_Sentiments.svg)\n",
    "\n",
    "##### Notes on the Colour Scheme and an Overall Picture of \"Happiness\" in America\n",
    "\n",
    "We note that a colour of yellow is associated with a happier sentiment, whereas a colour of blue with a sadder one (playing on the old stereotype of sunny = happy and blue = sad). \n",
    "\n",
    "Based on the pictures above, the South seems to be in the most negative state of mind, whereas the West Coast seems to be the happiest. The latter seems to be a affirmation of a social stereotype of places like California being happier than the rest of USA. \n",
    "\n",
    "The East Coast and Mid-West seems to be happier during the day (Type 3 weather) than at night (Type 1 weather), seemingly corroborating what was observed in the analysis of the circadian rhythm study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models of the Sentiments -- Positive, Negaive, and Neutral\n",
    "\n",
    "We look at using Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorisation (NMF) topic models of positive, negative, and neutral tweets. We do not analyse location-specific details here, but merge the positive, negative, and neutral tweets from various locations into single data files.\n",
    "\n",
    "For brevity, we look only at the NMF models here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Negative Topics (NMF)...\n",
      "Topic #0:\n",
      "https new dead real bad miss lose video sorry fake\n",
      "Topic #1:\n",
      "rt say hell life day new fake niggas miss got\n",
      "Topic #2:\n",
      "fuck 12 zwy5p3pm2x chikin eat juice2wavy yes like wh0rex flying\n",
      "Topic #3:\n",
      "shit finna ain holy stay tired doing two__xs ivs3nov2gt clearly\n",
      "Topic #4:\n",
      "people think wrong say really buy right making smart black\n",
      "Topic #5:\n",
      "bitch real 4w7wdmlwsf leave tryna lil surprise juanii___g giooplbhf2 bout\n",
      "Topic #6:\n",
      "just want did leave wanna tryna remember 4w7wdmlwsf said picture\n",
      "Topic #7:\n",
      "trump donald twitter mr ban president blocking clowns stephenking blocked\n",
      "Topic #8:\n",
      "amp man realdonaldtrump burning getting played thing game jersey ridiculous\n",
      "Topic #9:\n",
      "ass yo like finna dead tired named met girl fat\n",
      "Topic #10:\n",
      "im crying anime opening spongebob q8ew4evuui marynicolexx going rt ugly\n",
      "Topic #11:\n",
      "don like want niggas let really bother come hoes understand\n",
      "Topic #12:\n",
      "stop need watching pics wyd multiple playing didn girl drop\n",
      "Topic #13:\n",
      "fucking kfvilj1ax1 aghcid food hell headline week rest saiahz thi\n",
      "Topic #14:\n",
      "woman women years took 100 literally believe false rape accusations\n",
      "Topic #15:\n",
      "hate like realdonaldtrump love does media bro lol feeling man\n",
      "Topic #16:\n",
      "damn right really god virgo scared ur ain thought album\n",
      "Topic #17:\n",
      "luv rage uzi lil dropped vert tonight dropping album cozy\n",
      "Topic #18:\n",
      "going know exactly wtf im ones tf yea didn real\n",
      "Topic #19:\n",
      "time limited august 2017 wasted getting date say break ve\n",
      "()\n",
      "# Positive Topics (NMF)...\n",
      "Topic #0:\n",
      "https free video youtube beautiful help okay ready 10 shop\n",
      "Topic #1:\n",
      "rt trump people win beautiful time help today friends president\n",
      "Topic #2:\n",
      "love tell nowplaying brother say people ve shit life yrs\n",
      "Topic #3:\n",
      "like looks feel look nigga really people trying spoil sounds\n",
      "Topic #4:\n",
      "good morning luck day time today gas thing looks really\n",
      "Topic #5:\n",
      "thank spread sharing support day realdonaldtrump following honor appreciate twitter\n",
      "Topic #6:\n",
      "want know really jobs job work hiring million click details\n",
      "Topic #7:\n",
      "amp follow win retweet chance friends men free realdonaldtrump body\n",
      "Topic #8:\n",
      "lol got know exactly let man gone did guess bitch\n",
      "Topic #9:\n",
      "happy birthday day life live hope makes thursday make lil\n",
      "Topic #10:\n",
      "thanks latest follow sharing following hey daily friend guys special\n",
      "Topic #11:\n",
      "best friend thing life kingjames video ve hands business decide\n",
      "Topic #12:\n",
      "don know make people think care trust really forget need\n",
      "Topic #13:\n",
      "just going wanna got ve really time fine doing little\n",
      "Topic #14:\n",
      "great job time fit day hiring today interested careerarc work\n",
      "Topic #15:\n",
      "new video taylor swift song music york favorite gas check\n",
      "Topic #16:\n",
      "yes works law literally realdonaldtrump omg oh did baby watch\n",
      "Topic #17:\n",
      "album 12 tonight liluzivert ready rt 17 enjoy release getting\n",
      "Topic #18:\n",
      "god oh amazing did capitalize bless day pray thing man\n",
      "Topic #19:\n",
      "better make worldwide lives million legal online feel getting hope\n",
      "()\n",
      "# Neutral Topics (NMF)...\n",
      "Topic #0:\n",
      "https check nowplaying watch shop live 2017 youtube mood game\n",
      "Topic #1:\n",
      "rt https mood texas ll days look realdonaldtrump follow tomorrow\n",
      "Topic #2:\n",
      "just posted photo tl wanna trying did 20 times really\n",
      "Topic #3:\n",
      "new video music york single song check taylorswift13 model posted\n",
      "Topic #4:\n",
      "time mws5dddypw reignofapril ya real living remember days lordflaconegro scene\n",
      "Topic #5:\n",
      "don say ya reason sleep live house throw high believe\n",
      "Topic #6:\n",
      "amp class copied syllabus plagiarism pasted saraheinecke94 section info merch\n",
      "Topic #7:\n",
      "day school come game man start tomorrow make actually ve\n",
      "Topic #8:\n",
      "got ve man finally list hit long work red said\n",
      "Topic #9:\n",
      "need dont asap man gone does relationship 10 sleep year\n",
      "Topic #10:\n",
      "trump president donald russia realdonaldtrump gop obama twitter charlottesville wall\n",
      "Topic #11:\n",
      "know means didn doing did oh girl real doesn things\n",
      "Topic #12:\n",
      "people vs morning checked automatically slutropico 9jdj7qfzh8 followed unfollowed black\n",
      "Topic #13:\n",
      "right gxngstawifi kcawdmjxgf dis thing ll live come hurricaneharvey houston\n",
      "Topic #14:\n",
      "gt humble com lt country mixtape download large season air\n",
      "Topic #15:\n",
      "life real story home short trying ve goes summer way\n",
      "Topic #16:\n",
      "let ll make reason head way ton home surgery bunch\n",
      "Topic #17:\n",
      "today week ago year work days paid away provide coming\n",
      "Topic #18:\n",
      "going tonight family country tech thinking watch doing school mind\n",
      "Topic #19:\n",
      "think nigga character kfkb_ 8tgncqw5ld cat large literally store real\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "=====================================================\n",
    "\tAnalysis of Textual Data from Twitter\n",
    "=====================================================\n",
    "'''\n",
    "\n",
    "def load_text_data_to_list(data_file_name):\n",
    "\tdata_list = []\n",
    "\tif path.exists(getcwd()+'/'+data_file_name):\n",
    "\t\tdata_file = open(getcwd()+'/'+data_file_name, 'r')\n",
    "\t\tfor line in data_file:\n",
    "\t\t\tdata_list.append(line)\n",
    "\t\tdata_file.close()\n",
    "\telse:\n",
    "\t\tprint \"# Warning [load_text_data_to_list(...)]: File \\'\"+data_file_name+\"\\' not found in current directory (\"+getcwd()+')'\n",
    "\treturn data_list\n",
    "\n",
    "list_neg = load_text_data_to_list(\"neg.txt\")\n",
    "list_pos = load_text_data_to_list(\"pos.txt\")\n",
    "list_neu = load_text_data_to_list(\"neu.txt\")\n",
    "\n",
    "def process_nmf(data_list, num_features=1000, num_topics=20, num_top_words=10):\n",
    "\tnmf_model_instance, nmf_feature_names = nmf_model(data_list, num_features, num_topics)\n",
    "\tprint_top_words(nmf_model_instance, nmf_feature_names, num_top_words)\n",
    "\n",
    "def process_lda(data_list, num_features=1000, num_topics=20, num_top_words=20):\n",
    "\tlda_model_instance, lda_feature_names = lda_model(data_list, num_features, num_topics)\n",
    "\tprint_top_words(lda_model_instance, lda_feature_names, num_top_words)\n",
    "\t\n",
    "print \"# Negative Topics (NMF)...\"\n",
    "process_nmf(list_neg)\n",
    "print \"# Positive Topics (NMF)...\"\n",
    "process_nmf(list_pos)\n",
    "print \"# Neutral Topics (NMF)...\"\n",
    "process_nmf(list_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Pop Star, A President, and a Hurricane -- Topic Models of a Nation's Tweets\n",
    "\n",
    "Negative tweets seem to consist of popular culture items -- including what seems to be pornographic references =S -- and some political rants. Popular culture and politics also seem to be key contributors to positive-sentiment tweets. The notable difference in the neutral-sentiment tweets has to do with the presence of the topic of Hurricane Harvey.\n",
    "\n",
    "In fact, Hirrican Harvey could be a factor behind the South feeling more negative than the rest of the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Flask App for Presenting Topic Models by City\n",
    "\n",
    "We realise there is simply too much data to be presented for topic models of tweets. To make matters more tractable, we opt to create an HTML file for every city using a small Python sript, such that we can use the resultant files to create a small web app in Flask. \n",
    "\n",
    "We incorporate the following items into each HTML file:\n",
    "    1. Word clouds for the positive, negative, and neutral tweets for each city\n",
    "    2. NMF and LDA topic model results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "============================================================\n",
    "\tHTML Pages for Topic Model Data Presentation\t\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "from AncilliaeHTML import *\n",
    "\n",
    "def process_nmf_html(html_file, h2_text, data_list, num_features=1000, num_topics=20, num_top_words=10):\n",
    "\tnmf_model_instance, nmf_feature_names = nmf_model(data_list, num_features, num_topics)\n",
    "\tadd_html_h2(html_file, h2_text)\n",
    "\ttop_words_html(nmf_model_instance, nmf_feature_names, num_top_words, html_file)\t\n",
    "\n",
    "def process_lda_html(html_file, h2_text, data_list, num_features=1000, num_topics=20, num_top_words=10):\n",
    "\tlda_model_instance, lda_feature_names = lda_model(data_list, num_features, num_topics)\n",
    "\tadd_html_h2(html_file, h2_text)\n",
    "\ttop_words_html(lda_model_instance, lda_feature_names, num_top_words, html_file)\t\n",
    "\n",
    "def get_text(data_file_name):\n",
    "\tif path.exists(getcwd()+'/'+data_file_name):\n",
    "\t\treturn open(getcwd()+'/'+data_file_name).read()\n",
    "\treturn None\n",
    "\n",
    "def create_html_dir(dir_structure):\n",
    "\tif dir_structure[0] == '/':\n",
    "\t\tdir_structure = dir_structure[1:]\n",
    "\tif dir_structure[-1] == '/':\n",
    "\t\tdir_structure = dir_structure[:-1]\n",
    "\tdir_names = dir_structure.split('/')\n",
    "\tcurrent_dir = getcwd()\n",
    "\tfor dir_name in dir_names:\n",
    "\t\tcurrent_dir += '/' + dir_name\n",
    "\t\tif not(path.exists(current_dir)):\n",
    "\t\t\tmkdir(current_dir)\n",
    "\n",
    "flask_app_main_dir = \"/FlaskApp/\"\n",
    "flask_app_pages_subdir = \"pages/\"\n",
    "flask_app_images_subdir = \"images/\"\n",
    "flask_app_pages_dir = flask_app_main_dir + flask_app_pages_subdir\n",
    "flask_app_images_dir = flask_app_main_dir + flask_app_pages_subdir + flask_app_images_subdir\n",
    "\n",
    "create_html_dir(flask_app_pages_dir)\n",
    "create_html_dir(flask_app_images_dir)\n",
    "\n",
    "index_html_file_name = \"TopicIndex.html\"\n",
    "index_html_file = init_html_file(getcwd()+flask_app_main_dir+index_html_file_name, \"Index Page -- Tweet Sentiment Topics by City\")\t\n",
    "\n",
    "for city in dict_city_coordinate.keys():\n",
    "\tif city == \"san Jose\":\n",
    "\t\tcontinue\n",
    "\tname_prefix = city.replace(' ', '_').replace(',', '')\n",
    "\t\n",
    "#\tflask_app_pages_dir = flask_app_pages_dir+name_prefix\n",
    "\t\t\n",
    "\t\n",
    "\tlist_neg = load_text_data_to_list(name_prefix+\"_full__neg.txt\")\n",
    "\tlist_pos = load_text_data_to_list(name_prefix+\"_full__pos.txt\")\n",
    "\tlist_neu = load_text_data_to_list(name_prefix+\"_full__neu.txt\")\n",
    "\n",
    "\ttext_neg = get_text(name_prefix+\"_full__neg.txt\")\n",
    "\ttext_pos = get_text(name_prefix+\"_full__pos.txt\")\n",
    "\ttext_neu = get_text(name_prefix+\"_full__neu.txt\") \n",
    "\n",
    "\tcity_html_file = init_html_file(getcwd()+flask_app_pages_dir+'/'+name_prefix, city)\n",
    "\n",
    "\tcreate_wordcloud_image(text_neg, getcwd()+flask_app_images_dir+'/'+\"WordCloud__neg__\"+name_prefix+\".png\")\n",
    "\tadd_html_h2(city_html_file, \"Word Cloud -- Negative Tweets\")\n",
    "\tadd_html_image(flask_app_images_subdir+\"WordCloud__neg__\"+name_prefix, city_html_file)\t#add_html_image(flask_app_images_subdir+\"WordCloud__neg__\"+name_prefix+\".png\", city_html_file)\n",
    "\tcreate_wordcloud_image(text_pos, getcwd()+flask_app_images_dir+'/'+\"WordCloud__pos__\"+name_prefix+\".png\")\n",
    "\tadd_html_h2(city_html_file, \"Word Cloud -- Positive Tweets\")\n",
    "\tadd_html_image(flask_app_images_subdir+\"WordCloud__pos__\"+name_prefix, city_html_file)\t# add_html_image(flask_app_images_subdir+\"WordCloud__pos__\"+name_prefix+\".png\", city_html_file)\n",
    "\tcreate_wordcloud_image(text_neu, getcwd()+flask_app_images_dir+'/'+\"WordCloud__neu__\"+name_prefix+\".png\")\n",
    "\tadd_html_h2(city_html_file, \"Word Cloud -- Neutral Tweets\")\n",
    "\tadd_html_image(flask_app_images_subdir+\"WordCloud__neu__\"+name_prefix, city_html_file)\t# add_html_image(flask_app_images_subdir+\"WordCloud__neu__\"+name_prefix+\".png\", city_html_file)\n",
    "\n",
    "\tprocess_nmf_html(city_html_file, \"NMF Model -- Negative Tweets\", list_neg)\n",
    "\tprocess_nmf_html(city_html_file, \"NMF Model -- Positive Tweets\", list_pos)\n",
    "\tprocess_nmf_html(city_html_file, \"NMF Model -- Neutral Tweets\", list_neu)\n",
    "\n",
    "\tprocess_lda_html(city_html_file, \"LDA Model -- Negative Tweets\", list_neg)\n",
    "\tprocess_lda_html(city_html_file, \"LDA Model -- Positive Tweets\", list_pos)\n",
    "\tprocess_lda_html(city_html_file, \"LDA Model -- Neutral Tweets\", list_neu)\n",
    "\t\n",
    "\tend_html_file(city_html_file)\n",
    "\tadd_html_link(index_html_file, flask_app_pages_subdir+name_prefix, city)\n",
    "\n",
    "end_html_file(index_html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Word Cloud Image -- Neutral-Sentiment Tweets for Houston\n",
    "\n",
    "We present a sample word cloud for Houston, TX, where a visual representation is provided for the words used in neutral-sentiment tweets recorded for that city.\n",
    "\n",
    "![title](FlaskApp/pages/images/WordCloud__neu__houston.png)\n",
    "\n",
    "As may be expected, Hurricane Harvey seems to be taking centre-stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discussion -- Categories of Weather in USA, Impacts of Circadian Rhythm on Tweet Sentiment, Regional Variations of Positivity/Negativity, and Hot Topics\n",
    "\n",
    "Here, we attempt to answer the questions we had set out to investigate \n",
    "\n",
    "    1. We find that weather in the United States could be categorised, using a k-Means Clustering Method, into four different classes for August 24th & 25th, 2017:\n",
    "        * We further find that these correspond roughly to three different geographical regions, which themselves can be obtained via k-Means Clusters.\n",
    "        * There seems to be five principal components of weather data based on standard Principal Component Analysis, which we interpret to form three distinct classes:\n",
    "            1. Perceived temperature, including effects of humidity and pressure\n",
    "            2. Evaporative effects of sunlight and their impact on moisture in the atmosphere\n",
    "            3. Three different contributors to the dynamics of cloud formation.\n",
    "    2. We observe that once tweet sentiments are analysed in the context of a \"journal phase\" parameter (defined and explained above), the data seems to offer interesting insights into positive/negative-sentiment scores observed.\n",
    "        * Tweets seem to have the lowest sentiment scores in the middle of the night. There could be a correlation between darkness and depression: depressed people may be using social media later into the night, and their depression is most significant at the height of darkness. \n",
    "        * Sunset is associated with a plunge in tweet sentiment positivity.\n",
    "        * Sunrise seems to correspond to a peak in tweet sentiment positivity.\n",
    "    3. We also undertake analysis of the geographical distribution of negative-sentiment tweets:\n",
    "        * People on the West Coast seem to be marginally happier.\n",
    "        * People on the East Coast are happier during the day.\n",
    "        * People in the South are not as happy, perhaps, due to the effect of Hurricane Harvey (as noted from textual analysis).\n",
    "    4. Upon applying LDA and NMF topic models to the tweet data, we notice three topics that immediately stand out:\n",
    "        * President Donald Trump, who has a well-known Twitter presence\n",
    "        * Taylor Swift, the pop star who had announced a new album one day ago\n",
    "        * Hurricane Harvey, which was pounding the coast of Texas during that time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion -- Towards Using Data Science as a Tool of Modern Psychology and Psychiatry\n",
    "\n",
    "The challenges posed by mental health issues have been part of the story of humankind for a very long time. The progress of science has helped shape the landscape of psychology and psychaitry throughout human history. From the witch doctors of prehistory, to the ancient Greek and Roman philosophers, to the modern-day figures like Sigmund Freud and Alice Miller, psychology has taken a convoluted course into becoming a science from a pseudo-science. In these exciting times when Data Science is helping quantify the often-qualitative nuances of the Social Sciences, it is hoped that psychology and psychiatry will benefit from psyco-metric analysis, whereby diagnostic and interventional challenges facing modern-day mental health issues can be conquered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
